#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time     :2020/7/17 18:57
# @Author   :DKJ
# @File     :meizitu_done.py
# @Software :PyCharm

'''
â¤ï¸â¤ï¸ èƒŒæ™¯ç®€ä»‹ â¤ï¸â¤ï¸
 ğŸˆå¦¹å­å›¾:ğŸˆ
    æœ‰ä¸¤ä¸ª:  å›¾ç‰‡éƒ½ä¸€æ ·.
    www.meizitu.com æ²¡å¹¿å‘Š, ä¸€é¡µå¤šå¼ ç…§ç‰‡. ä¸å¥½çˆ¬
    www.mzitu.com   å¹¿å‘Šå¤š, ä¸€é¡µä¸€å¼ ç…§ç‰‡. æœ‰æ°´å°,å®¹æ˜“çˆ¬
    å…ˆæ¥ç®€å•çš„ï¼Œäºæ˜¯çˆ¬äº†www.mzitu.com
 ğŸˆç½‘ç«™åˆ†æğŸˆ:
     å¦¹å­å›¾å‡ ä¹æ¯å¤©éƒ½æ›´æ–°ï¼Œ
     åˆ°ç°åœ¨ä¸ºæ­¢æœ‰ 140é¡µ
     æ¯é¡µ24ä¸ªä¸»é¢˜å†™çœŸï¼Œ
     æ¯ä¸ªä¸»é¢˜ä¸‹æœ‰å‡ åå¼ ç…§ç‰‡ï¼Œæ¯å¼ ç…§ç‰‡ä¸€ä¸ªç½‘é¡µ.
     ç½‘é¡µç»“æ„ç®€å•.ç”¨ BeautifulSoup å°±å¯ä»¥è½»æ¾çˆ¬å–ã€‚
    ğŸ“Œ ç½‘ç«™140+é¡µ.  æ¯é¡µçš„ç½‘å€å¾ˆæœ‰è§„å¾‹ 1-140
        åªè¦èƒ½è·å¾—ä¸€ä¸ªé¡µé¢é‡Œé¢çš„æ•°æ®
        å‰©ä¸‹é¡µé¢çš„æ•°æ®åªè¦ä»1åˆ°140 å¾ªç¯.å°±å¯ä»¥äº†
        http://www.mzitu.com/page/1
        http://www.mzitu.com/page/2
        http://www.mzitu.com/page/3
        ......
        http://www.mzitu.com/page/140
    ğŸ“Œ æ¯é¡µ24ä¸ªä¸»é¢˜. æ¯ä¸ªä¸»é¢˜ä¸€ä¸ªé“¾æ¥.
        http://www.mzitu.com/87933
        http://www.mzitu.com/87825
        æ¯ä¸ªä¸»é¢˜ä¹‹é—´å°±æ²¡ä»€ä¹ˆè”ç³»äº†.
        æ‰€æœ‰ä¸»é¢˜çš„ç½‘å€å°±å¾—æ‰‹åŠ¨çˆ¬ä¸‹æ¥.
        è¿™é‡Œå°±ä¸èƒ½ç”¨å¾ªç¯äº†...
    ğŸ“Œ æ¯ä¸ªä¸»é¢˜è¯ºå¹²å¼ å›¾ç‰‡. æ¯å¼ å›¾ç‰‡ä¸€ä¸ªç½‘å€
        http://www.mzitu.com/86819/1
        http://www.mzitu.com/86819/2
        http://www.mzitu.com/86819/3
        å•ä¸ªä¸»é¢˜ä¸‹çš„å›¾ç‰‡å¾ˆæœ‰è§„å¾‹
        åªè¦çŸ¥é“è¿™ä¸ªä¸»é¢˜çš„å›¾ç‰‡æ•°é‡å°±èƒ½å¾ªç¯å‡ºæŸä¸»é¢˜ä¸‹æ‰€æœ‰çš„ç½‘å€.
        è¿™ä¸ªç½‘å€ ä¸ç­‰äº å›¾ç‰‡çš„ç½‘å€.
        å›¾ç‰‡ç½‘å€ éœ€è¦åˆ°æ¯ä¸ªç½‘é¡µä¸‹é¢åŒ¹é…å‡ºæ¥.
 ğŸˆçˆ¬è™«æ­¥éª¤ï¼šğŸˆ
     æ•´ä¸ªå¦¹å­å›¾æ‰€æœ‰ä¸»é¢˜çš„ç½‘å€.            get_page1_urls
     æŸä¸»é¢˜ä¸‹ç¬¬ä¸€å¼ ç…§ç‰‡åœ°å€               get_img_url
     æŸä¸»é¢˜çš„ç…§ç‰‡æ•°                       get_page_num
     ç”¨å¾ªç¯è·å–æŸä¸»é¢˜ä¸‹æ‰€æœ‰ç…§ç‰‡åœ°å€       get_img_url
     è·å–å„ä¸ªä¸»é¢˜çš„ä¸»é¢˜åå­—               get_img_title
     ä¸‹è½½æ‰€æœ‰ä¸»é¢˜ä¸‹çš„æ‰€æœ‰ç…§ç‰‡             download_imgs
'''

# â¤ï¸â¤ï¸ â†“â†“â†“ 0: ä¾èµ–æ¨¡å— â†“â†“â†“ â¤ï¸â¤ï¸


import urllib.request          # è·å–ç½‘é¡µå†…å®¹
from bs4 import BeautifulSoup  # è§£æç½‘é¡µå†…å®¹
import re                      # æ­£åˆ™å¼æ¨¡å—.
import os                      # ç³»ç»Ÿè·¯å¾„æ¨¡å—: åˆ›å»ºæ–‡ä»¶å¤¹ç”¨
import socket                  # ä¸‹è½½ç”¨åˆ°
import time                    # ä¸‹è½½ç”¨åˆ°

# â¤ï¸â¤ï¸ â†“â†“â†“ è·å–æ•´ä¸ªå¦¹å­ç½‘æ‰€æœ‰çš„ä¸»é¢˜ â†“â†“â†“ â¤ï¸â¤ï¸


def get_page1_urls():          # å®šä¹‰ä¸€ä¸ªå‡½æ•°
    page1_urls = []            # å®šä¹‰ä¸€ä¸ªæ•°ç»„,æ¥å‚¨å­˜æ‰€æœ‰ä¸»é¢˜çš„URL
    for page in range(1, 2):
        # 1-140. æ•´ä¸ªå¦¹å­å›¾åªæœ‰140é¡µ,æ³¨æ„ä¸‹é¢ç¼©è¿›å†…å®¹éƒ½åœ¨å¾ªç¯å†…çš„!
        url = 'http://www.mzitu.com/page/' + str(page)
        request = urllib.request.Request(url)
        # åˆ¶ä½œè¯·æ±‚å¤´äº†. 140é¡µ æ¯é¡µéƒ½è¯·æ±‚ä¸€é. è‡ªç„¶å°±èƒ½è·å–åˆ°æ¯é¡µä¸‹çš„24ä¸ªä¸»é¢˜äº†
        html = urllib.request.urlopen(request, timeout=20).read()
        # read å°±æ˜¯è¯»å–ç½‘é¡µå†…å®¹å¹¶å‚¨å­˜åˆ° htmlå˜é‡ä¸­.
        soup = BeautifulSoup(html, 'lxml')
        # æŠŠä¸‹è½½çš„ç½‘é¡µ.ç»“æ„åŒ–æˆDOM, æ–¹ä¾¿ä¸‹é¢ç”¨ find å–å‡ºæ•°æ®
        lis = soup.find('ul', {'id': 'pins'}).find_all('li')
        # æ‰¾åˆ° id ä¸ºpins è¿™ä¸ªåˆ—è¡¨ä¸‹é¢çš„ æ¯ä¸ªåˆ— å°±æ‰¾åˆ°æ¯ä¸ªé¡µé¢ä¸‹çš„ 24ä¸ªä¸»é¢˜äº†
        for li in lis:
            # éå†æ¯é¡µä¸‹é¢çš„24ä¸ªä¸»é¢˜ (ä¹Ÿå°±æ˜¯24ä¸ªli)
            page1_urls.append(li.find('a')['href'])
            # æŠŠæ¯ä¸ªä¸»é¢˜çš„åœ°å€. æ·»åŠ åˆ°page1_urls è¿™ä¸ªæ•°ç»„é‡Œé¢.
        # print(page1_urls)
        # # æ˜¾ç¤ºç½‘å€. æµ‹è¯•ç”¨. å¾ªç¯140æ¬¡. è¿™æ ·å°±è·å¾—äº†æ‰€æœ‰ä¸»é¢˜çš„ç½‘å€äº†
    return page1_urls

# â¤ï¸â¤ï¸ â†“â†“â†“ è‡ªåŠ¨è·å–æŸä¸»é¢˜çš„ç…§ç‰‡æ•°é‡ â†“â†“â†“ â¤ï¸â¤ï¸
# è¿›å…¥æŸä¸ªä¸»é¢˜, ç„¶ååˆ†æåº•éƒ¨çš„ å¯¼èˆªæ¡.
# å¯¼èˆªæ¡æ ¼å¼: ä¸Šä¸€ç»„ 1 2 3 4 ... 64 ä¸‹ä¸€ç»„
# å¾ˆå¤šæŒ‰é’®.æ¯ä¸ªæŒ‰é’®éƒ½æ˜¯ä¸€ä¸ª<a>å…ƒç´ .
# å€’æ•°ç¬¬äºŒä¸ª<a>å…ƒç´  è¿™é‡Œä¹Ÿå°±æ˜¯64 å°±æ˜¯ç…§ç‰‡æ•°é‡!

def get_page_num(page1_url):        # å‚æ•° page1_url ä¸ä¸€å®šè¦å¤–ç•Œä¼ å…¥çš„. å¯ä»¥ç»™å‡½æ•°é‡Œé¢ç”¨çš„.
    request = urllib.request.Request(page1_url)
    try:
        html = urllib.request.urlopen(request, timeout=20).read()
    except:
        try:
            html = urllib.request.urlopen(request, timeout=20).read()
        except:
            return None
            # è¿™ä¸ªå‡½æ•°ä¼šé‡å¤è¯·æ±‚ä¸¤æ¬¡. å¦‚æœä¸¤æ¬¡éƒ½è¶…æ—¶å°±æ”¾å¼ƒ.
    soup = BeautifulSoup(html, 'lxml')
    try:
        page_num = soup.find('div', {'class': 'pagenavi'}).find_all('a')[-2].find('span').get_text()
    except:
        return None
    return int(page_num)

# â¤ï¸â¤ï¸ ä¸‰: è·å–æŸä¸»é¢˜ä¸‹ç¬¬ä¸€å¼ ç…§ç‰‡çš„URL. â¤ï¸â¤ï¸
# ç»“åˆä¸Šé¢çš„ç…§ç‰‡æ•°é‡. å°±èƒ½è·å–åˆ°æŸä¸»é¢˜ä¸‹çš„æ‰€æœ‰ç…§ç‰‡é“¾æ¥äº†.


def get_img_url(url):
    request = urllib.request.Request(url)
    try:
        html = urllib.request.urlopen(request, timeout=20).read()
    except:
        try:
            html = urllib.request.urlopen(request, timeout=20).read()
        except:
            return None
    soup = BeautifulSoup(html, 'lxml')
    try:
        img_url = soup.find(
            'div', {'class':
                    'main-image'}).find('p').find('a').find('img')['src']
    except:
        return None
    return img_url

def get_img_urls(page1_url):
    page_num = get_page_num(page1_url)
    # è¿™é‡Œå°±ç”¨åˆ°äº† ä¸Šé¢çš„ get_page_num è¿™ä¸ªå‡½æ•°äº†.
    if page_num is None:
        return None
    img_urls = []
    # å®šä¹‰ä¸€ä¸ªæ•°ç»„ æ¥å‚¨å­˜è¯¥ä¸»é¢˜ä¸‹çš„ æ‰€æœ‰ç…§ç‰‡çš„ URL
    for page in range(1, page_num + 1):
        url = page1_url + '/' + str(page)
        # å®é™…ç…§ç‰‡çš„é“¾æ¥åœ°å€ å°±æ˜¯ä¸»é¢˜çš„é“¾æ¥ + / + æ•°é‡
        img_url = get_img_url(url)
        # è¿™é‡Œç”¨åˆ°äº† get_img_url è¿™ä¸ªå‡½æ•°. å¯ä»¥è·å–è¯¥ä¸»é¢˜ä¸‹çš„ç¬¬ä¸€å¼ ç…§ç‰‡.
        # ç°åœ¨æ˜¯åœ¨å¾ªç¯é‡Œé¢. å¾ªç¯æ¬¡æ•°å°±æ˜¯ è¯¥ä¸»é¢˜çš„ç…§ç‰‡æ•°é‡+1
        if img_url is None:
            return None
        else:
            img_urls.append(img_url)
        # æŠŠè·å–åˆ°çš„ url æ·»åŠ åˆ° img_urls è¿™ä¸ªæ•°ç»„é‡Œ.
        # è¿™æ ·å¾ªç¯ä¸‹æ¥ img_urls æ•°ç»„é‡Œé¢å°±æœ‰è¯¥ä¸»é¢˜ä¸‹çš„æ‰€æœ‰ç…§ç‰‡åœ°å€äº†
    return img_urls

# â¤ï¸â¤ï¸ äº”: è·å–æŸä¸»é¢˜åç§°,åˆ›å»ºæœ¬åœ°æ–‡ä»¶å¤¹ç”¨ â¤ï¸â¤ï¸


def get_img_title(page1_url):
    request = urllib.request.Request(page1_url)
    try:
        html = urllib.request.urlopen(request, timeout=20).read()
    except:
        try:
            html = urllib.request.urlopen(request, timeout=20).read()
        except:
            return None
    soup = BeautifulSoup(html, 'lxml')
    # <h2 class="main-title">å¤å…¸æ°”è´¨å‹ç¾å¥³æ–½è¯— é¡¶çº§ç¾è…¿åŠ é…¥èƒ¸åœ†è‡€ç«è¾£èº«ææ€§æ„Ÿåè¶³</h2>
    title = soup.find('h2', {'class': 'main-title'}).get_text()
    # ä¸‹é¢ä¸¤è¡Œæ˜¯å¼‚å¸¸åˆ†æ..
    removeSign = re.compile(r'[\/:*?"<>|]')
    # re å°±æ˜¯æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—
    # re.compile æŠŠæ­£åˆ™è¡¨è¾¾å¼å°è£…èµ·æ¥. å¯ä»¥ç»™åˆ«çš„å‡½æ•°ç”¨. ()é‡Œé¢çš„æ‰æ˜¯çœŸçš„ è¡¨è¾¾å¼.
    # r'[\/:*?"<>|]'
    # [] è¡¨ç¤ºä¸€ä¸ªå­—ç¬¦é›†;  \å¯¹åé¢çš„è¿›è¡Œè½¬ä¹‰ è‹±æ–‡/æ˜¯ç‰¹æ®Šç¬¦å·; å…¶ä»–çš„æ˜¯æ­£å¸¸ç¬¦å·.
    title = re.sub(removeSign, '', title)
    # re.sub åœ¨å­—ç¬¦ä¸²ä¸­ æ‰¾åˆ°åŒ¹é…è¡¨è¾¾å¼çš„æ‰€æœ‰å­ä¸². ç”¨å¦ä¸€ä¸ªè¿›è¡Œæ›¿æ¢.è¿™é‡Œç”¨'' å°±æ˜¯åˆ é™¤çš„æ„æ€.
    # å°±æ˜¯è¯´ åˆ é™¤æ ‡é¢˜é‡Œé¢çš„ /:*?"<>| è¿™äº›ç¬¦å·.
    # è‹±æ–‡åˆ›å»ºæ–‡ä»¶å¤¹æ—¶å€™ ä¸èƒ½æœ‰ç‰¹æ®Šç¬¦å·çš„!!!
    return title

# â¤ï¸â¤ï¸ å…­: å®šä¹‰ä¸‹è½½æŸä¸»é¢˜æ‰€æœ‰å›¾ç‰‡çš„å‡½æ•° â¤ï¸â¤ï¸
# ä¸‹è½½è‚¯å®šè¦åˆ›å»ºæ–‡ä»¶å¤¹.è¦ç”¨åˆ°è·¯å¾„.è¿™å°±éœ€è¦ os æ¨¡å—äº†.
# æˆ‘ä»¬æŠŠç…§ç‰‡ å»ºç«‹ä¸ªæ–‡ä»¶å¤¹ ä¸‹è½½åˆ° è„šæœ¬è¿è¡Œçš„ç›®å½•ä¸‹
# os.pathæ¨¡å—ä¸»è¦ç”¨äºæ–‡ä»¶çš„å±æ€§è·å–ï¼Œç»å¸¸ç”¨åˆ°ï¼Œä»¥ä¸‹æ˜¯è¯¥æ¨¡å—çš„å‡ ç§å¸¸ç”¨æ–¹æ³•
# print(os.getcwd())                 # è·å–å¹¶è¾“å‡ºå½“å‰è„šæœ¬æ‰€åœ¨çš„ç›®å½•.
# os.mkdir('./å¦¹å­å›¾')               # åœ¨å½“å‰æ–‡ä»¶å¤¹ä¸‹ å»ºç«‹ å¦¹å­å›¾ æ–‡ä»¶å¤¹.
# os.rmdir('./å¦¹å­å›¾')               # åœ¨å½“å‰æ–‡ä»¶å¤¹ä¸‹ åˆ é™¤ å¦¹å­å›¾ æ–‡ä»¶å¤¹.
# if os.path.exists('./å¦¹å­å›¾'):     # åˆ¤æ–­å½“å‰æ–‡ä»¶å¤¹ æ˜¯å¦å­˜åœ¨   å¦¹å­å›¾è¿™ä¸ªæ–‡ä»¶å¤¹
# if not os.path.exists('./å¦¹å­å›¾'): # åˆ¤æ–­å½“å‰æ–‡ä»¶å¤¹ æ˜¯å¦ä¸å­˜åœ¨ å¦¹å­å›¾è¿™ä¸ªæ–‡ä»¶å¤¹
# æœ¬é¡¹ç›®æˆ‘ä»¬å…ˆåˆ¤æ–­å½“å‰è„šæœ¬æ–‡ä»¶å¤¹ æ˜¯å¦å·²ç»æœ‰å¦¹å­å›¾è¿™ä¸ªæ–‡ä»¶å¤¹å­˜åœ¨.
# å¦‚æœä¸å­˜åœ¨é‚£å°±æ–°å»ºä¸€ä¸ªå¦¹å­å›¾æ–‡ä»¶å¤¹.
# å†åˆ¤æ–­å¦¹å­å›¾æ–‡ä»¶å¤¹ä¸‹ æœ‰æ²¡æœ‰å¯¹åº”çš„å­æ–‡ä»¶å¤¹å­˜åœ¨.


def download_imgs(page1_url):
    img_urls = get_img_urls(page1_url)
    if img_urls is None:
        return None
    if not os.path.exists('./å¦¹å­å›¾'):
        os.mkdir('./å¦¹å­å›¾')
    title = get_img_title(page1_url)
    if title is None:
        return
    local_path = './å¦¹å­å›¾/' + title
    if not os.path.exists(local_path):
        try:
            os.mkdir(local_path)
        except:
            pass
    if img_urls is None or len(img_urls) == 0:
        return
    else:
        print('--å¼€å§‹ä¸‹è½½' + title + '--')
        for img_url in img_urls:
            img_name = os.path.basename(img_url)
            print('æ­£åœ¨ä¸‹è½½ ' + img_name)
            print('from ' + img_url)
            socket.setdefaulttimeout(10)
            try:
                urllib.request.urlretrieve(img_url, local_path + '/' + img_name)
            except:
                print('ä¸‹è½½' + img_name + 'å¤±è´¥')
        print('--' + title + 'ä¸‹è½½å®Œæˆ--')


# â¤ï¸â¤ï¸ ä¸ƒ: ä¸‹è½½æ‰€æœ‰ä¸»é¢˜çš„å›¾ç‰‡ â¤ï¸â¤ï¸


def craw_meizitu():
    page1_urls = get_page1_urls()
    # è¿™é‡Œç”¨åˆ°äº† ç¬¬ä¸€ä¸ªå‡½æ•°. ä¹Ÿå°±æ˜¯è·å–æ‰€æœ‰ä¸»é¢˜çš„ URL.
    if page1_urls is None or len(page1_urls) == 0:
        return
    else:
        for page1_url in page1_urls:
            # å¾ªç¯ç¬¬å…­æ­¥ æ¥ä¸‹è½½æ‰€æœ‰ä¸»é¢˜çš„URL
            download_imgs(page1_url)


def main():
    craw_meizitu()
if __name__ == '__main__':
    main()